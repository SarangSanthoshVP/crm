#just cheking wheather chnages are worked or not
from langgraph.graph import StateGraph
from langgraph.prebuilt import create_react_agent
from langchain.agents import create_openai_tools_agent
from typing import TypedDict
from langchain_ollama import ChatOllama
from langchain.agents import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor
from sentence_transformers import SentenceTransformer
from pydantic import BaseModel,Field
from langchain_core.output_parsers import StrOutputParser
import faiss
import psycopg2
import pandas as pd
import os

# -------------------
# Agent State 
# -------------------

# Define the structure of the agent's state
class AgentState(TypedDict):
    user_input: str
    response: str

class SQLQuery(BaseModel):
    sql: str= Field(
        description="A valid SQL query that can be executed directly on the crm_leads table."
    )
# -------------------
# Database Connect        
# ------------------
def connect_db():
    return psycopg2.connect(
        dbname="CRM",
        user="postgres",
        password="12345",
        host="localhost",
        port="5432"
    )

def fetch_crm_data():
    conn = connect_db()
    df = pd.read_sql("SELECT * FROM crm_leads;", conn)
    conn.close()
    df['content'] = (
        df['lead_name'].fillna('').astype(str) + " " +
        df['customer'].fillna('').astype(str) + " " +
        df['hot_lead'].fillna('').astype(str) + " " +
        df['type'].fillna('').astype(str) + " " +
        df['account'].fillna('').astype(str) + " " +
        df['products'].fillna('').astype(str) + " " +
        df['team_name'].fillna('').astype(str) + " " +
        df['created_by'].fillna('').astype(str) + " " +
        df['lead_stage'].fillna('').astype(str) 
    )
    return df


# -------------------
@st.cache_resource
def build_faiss_index():
    df = fetch_crm_data()
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(df['content'].tolist(), convert_to_numpy=True, normalize_embeddings=True)  
    # normalize_embeddings=True ensures each vector has norm=1
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)  # inner product
    index.add(embeddings)
    return df, model, index

crm_df, embed_model, faiss_index = build_faiss_index()

def search_crm(query, top_n=5):
    query_vec = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    D, I = faiss_index.search(query_vec, top_n)
    results = crm_df.iloc[I[0]].copy()
    results['score'] = D[0]  # inner product = cosine similarity
    return results

# -------------------
# LLM
# -------------------
llm = ChatOllama(model="llama3.2")

# -------------------
# TOOLS
# -------------------
@tool(return_direct=True)
def greeting_tool(user_input: str) -> str:
    """Respond to greetings like hi, hello, etc. Use this when user says hello or similar greetings."""
    prompt = f"""You are a friendly chatbot. Respond warmly to greetings.
    User: {user_input}
    Response:"""
    result = llm.invoke(prompt)
    return result.content if hasattr(result, 'content') else str(result)

@tool(return_direct=True)
def rag_tool(user_input:str) -> str:
    """Use this ONLY for semantic or fuzzy queries (e.g., similar leads, descriptive search).
    Example: 'Show me hot leads related to social media.' 
    Do NOT use this for lead IDs, numeric filters, or aggregations."""

    search_results = search_crm(user_input, top_n=5)
    if search_results.empty:
        return "No Relevant CRM data found."
    
    context = ""
    for _, row in search_results.iterrows():
        context += f"""Lead ID:{row['lead_unique_id']} | Lead:{row['lead_name']} | Customer:{row['customer']} 
        | Hot lead:{row['hot_lead']} | Type:{row['type']} | Forecast Probability:{row['forecast_probability']} 
        | Forecast Period:{row['forecast_period']} | Contact:{row['contact']} | Created Date:{row['created_date']} 
        | Account:{row['account']} | Products:{row['products']} | Team Name:{row['team_name']} 
        | Created By:{row['created_by']} | Lead Revenue:{row['lead_revenue']} | Lead Stage:{row['lead_stage']}\n"""

    prompt = f"""
    You are a CRM assistant. Use the CRM records below to answer the question.

    User Question: {user_input}

    CRM Records:
    {context}
    
    Provide a concise, natural answer (not a list).
    """
    results = llm.invoke(prompt)
    return results.content if hasattr(results, 'content') else str(results)

@tool(return_direct=True)
def structured_query_tool(user_input: str) -> str:
    """Use this ONLY for structured/numeric queries (lead_unique_id lookups, filtering by customer, 
    lead_stage, team_name, or aggregations like SUM, COUNT, AVG of lead_revenue). 
    Example: 'Who is the customer for lead id 655883?' 
    Example: 'Total revenue generated by Team A'.
    Translates the question into SQL and executes it, then returns a natural language answer.

    
    """

    conn = connect_db()
    cur = conn.cursor()

    sql_gen=llm.with_structured_output(SQLQuery)

    # --- Step 1: Ask LLM to generate SQL ---
    sql_prompt = f"""
    You are an expert SQL generator. 
    Convert the following user request into a **valid SQL query** on the table crm_leads.
    
    Constraints:
    - Output ONLY a SQL query inside the `sql` field.
    - No explanations, no markdown formatting.
    - Use only these columns:
      lead_unique_id, lead_name, customer, hot_lead, type, 
      forecast_probability, forecast_period, contact, created_date, 
      account, products, team_name, created_by, lead_revenue, lead_stage.

    User Request: {user_input}

    """


    try:
        sql_obj:SQLQuery=sql_gen.invoke(sql_prompt)
        sql_query=sql_obj.sql.strip()

        cur.execute(sql_query)
        rows=cur.fetchall()
        colnames=[desc[0] for desc in cur.description]
        df=pd.DataFrame(rows,columns=colnames)
        conn.close()

        if df.empty:
            return "No matching records found"
        
        result_text=df.to_string(index=False)


    
        explain_prompt = f"""
        You are a helpful CRM assistant.
        Convert the following SQL result into a natural answer to the user's request.

        User Question: {user_input}
        SQL Result:
        {result_text}

        Answer in a short, conversational style:
        """
        explain = llm.invoke(explain_prompt)
        return explain.content if hasattr(explain, 'content') else result_text

    except Exception as e:
        conn.close()
        #return rag_tool(user_input)
        return f"Error running SQL: {str(e)}"


# -------------------
# Agent Setup
# -------------------
tools1 = [greeting_tool, rag_tool, structured_query_tool]

prompt1 = ChatPromptTemplate.from_messages([
    ("system", """You are a CRM assistant.
    
    - Use **greeting_tool** for casual greetings.
    - Use **structured_query_tool** for:
        * Lead lookups by **lead ID** or **lead name**
        * Exact filtering by customer, team, lead_stage, etc.
        * Numeric aggregations (sum, average, count) like revenue questions
    - Use **rag_tool** for:
        * Semantic or fuzzy searches
        * When user asks for related leads, general descriptions, or when no exact ID is mentioned
    
    If unsure, prefer structured_query_tool for numeric questions.
    """),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])



agent = create_openai_tools_agent(llm=llm, tools=tools1, prompt=prompt1)
agent_executor = AgentExecutor(agent=agent, tools=tools1, verbose=True)

# -------------------
# LangGraph Workflow
# -------------------
def route_with_agent(state: AgentState) -> dict:
    try:
        response = agent_executor.invoke({"input": state["user_input"]})
        final_output = response.get("output", "No response.")
        return {"response": final_output}
    except Exception as e:
        return {"response": f"Error: {str(e)}"}

def format_response(state: AgentState) -> dict:
    return {"response": state["response"]}

workflow = StateGraph(AgentState)
workflow.add_node("agent_router", route_with_agent)
workflow.add_node("response_formatter", format_response)
workflow.set_entry_point("agent_router")
workflow.add_edge("agent_router", "response_formatter")
workflow.add_edge("response_formatter", "__end__")
app = workflow.compile()

# -------------------
# Streamlit UI
# -------------------
st.title("CRM Agentic Chatbot")
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ask me about CRM data or say hello"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    inputs = {"user_input": prompt, "response": ""}
    with st.spinner("Processing..."):
        result = app.invoke(inputs)
        response = result.get("response", "No response.")
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
 